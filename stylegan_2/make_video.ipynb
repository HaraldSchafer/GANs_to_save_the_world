{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355'\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from stylegan2_pytorch import StyleGAN2\n",
    "def get_model(\n",
    "    gpu,\n",
    "    data='./data',\n",
    "    results_dir='./results',\n",
    "    models_dir='./models', \n",
    "    name='default',\n",
    "    new=False,\n",
    "    load_from=-1,\n",
    "    image_size = 512,\n",
    "    network_capacity = 16,\n",
    "    transparent = False,\n",
    "    batch_size = 1,\n",
    "    gradient_accumulate_every = 1,\n",
    "    num_train_steps = 150000,\n",
    "    learning_rate = 1e-4,\n",
    "    lr_mlp = 0.1,\n",
    "    ttur_mult = 1.5,\n",
    "    num_workers =  None,\n",
    "    save_every = 1000,\n",
    "    generate = False,\n",
    "    generate_interpolation = False,\n",
    "    save_frames = False,\n",
    "    num_image_tiles = 8,\n",
    "    trunc_psi = 0.75,\n",
    "    fp16 = False,\n",
    "    cl_reg = False,\n",
    "    fq_layers = [],\n",
    "    fq_dict_size = 256,\n",
    "    attn_layers = [],\n",
    "    use_feats=True,\n",
    "    aug_prob = 0.,\n",
    "    dataset_aug_prob = 0.,\n",
    "):\n",
    "    using_ddp = False\n",
    "    model = StyleGAN2(\n",
    "        image_size,        \n",
    "        network_capacity = network_capacity,\n",
    "        transparent = transparent,\n",
    "        lr = learning_rate,\n",
    "        lr_mlp = lr_mlp,\n",
    "        ttur_mult = ttur_mult,\n",
    "        fp16 = fp16,\n",
    "        cl_reg = cl_reg,\n",
    "        fq_layers = fq_layers,\n",
    "        fq_dict_size = fq_dict_size,\n",
    "        attn_layers = attn_layers,\n",
    "        use_feats=use_feats,\n",
    "        using_ddp=using_ddp,\n",
    "        gpu=gpu\n",
    "    )\n",
    "\n",
    "    if using_ddp:\n",
    "        world_size = torch.cuda.device_count()\n",
    "        torch.distributed.init_process_group(backend='nccl', rank=gpu, world_size=world_size)\n",
    "    else:\n",
    "        gpu = 0\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.set_device(gpu)\n",
    "    load(model, models_dir, num=load_from)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def load(GAN, models_dir, dir_name='default', num=-1, gpu=0):\n",
    "    name = num\n",
    "    if num == -1:\n",
    "        file_paths = [p for p in Path(models_dir + '/' + dir_name).glob('model_*.pt')]\n",
    "        saved_nums = sorted(map(lambda x: int(x.stem.split('_')[1]), file_paths))\n",
    "        if len(saved_nums) == 0:\n",
    "            return\n",
    "        name = saved_nums[-1]\n",
    "        print(f'continuing from previous epoch - {name}')\n",
    "\n",
    "    map_location = {'cuda:%d' % 0: 'cuda:%d' % gpu}\n",
    "    print(str(models_dir + '/' + dir_name + '/' + f'model_{name}.pt'),)\n",
    "    load_data = torch.load(str(models_dir + '/' + dir_name + '/' + f'model_{name}.pt'), map_location=map_location)\n",
    "   # print(load_data['GAN'].keys())\n",
    "    d2 = {}\n",
    "    for k in load_data['GAN']:\n",
    "      d2[k.replace('.module', '')] = load_data['GAN'][k]\n",
    "\n",
    "    GAN.load_state_dict(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_model(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import noise, image_noise\n",
    "\n",
    "\n",
    "def gen_styles(GAN, feats):\n",
    "    torch.manual_seed(0)\n",
    "    layers = GAN.num_layers\n",
    "    n = noise(feats.shape[0], GAN.latent_dim)\n",
    "    w_space = GAN.S(n, feats)\n",
    "    styles = w_space[:,None,:].repeat(1,layers,1)\n",
    "    return styles\n",
    "    \n",
    "w_styles = gen_styles(m, torch.zeros((1, 1024)).cuda())\n",
    "n = image_noise(1, m.image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = m.GE(w_styles, 0*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "imshow(img.cpu().detach().numpy()[0].swapaxes(0,2).swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xx.uncommon.column_store import ColumnStoreReader, ColumnStoreWriter\n",
    "import random\n",
    "from xx.uncommon.utils import transform_img\n",
    "\n",
    "from common.transformations.camera import W as FULL_W                                                                                                                                                                                                                              \n",
    "from common.transformations.camera import H as FULL_H                                                                                                                                                                                                                              \n",
    "from common.transformations.model import MEDMODEL_INPUT_SIZE, medmodel_intrinsics\n",
    "from tools.lib.framereader import FrameIterator\n",
    "from common.transformations.camera import view_frame_from_device_frame as view_from_device\n",
    "\n",
    "\n",
    "SEG_LIST = '/home/batman/openpilot/xx/pipeline/route_lists/1m_seg_list_500k.txt'\n",
    "from xx.pipeline.require import convert_to_slash\n",
    "with open(SEG_LIST) as f:\n",
    "  segs3 = convert_to_slash(f.read().strip().split('\\n'))\n",
    "BASE_DIR = \"http://data-ssd.comma.life/model_features/df05d416-a2c6-407e-8013-3c6c34e62d27_800/\"\n",
    "\n",
    "target_name = random.choice(segs3)\n",
    "#target_name ='36e84c795c78116c|2019-07-17--15-53-07/6'\n",
    "\n",
    "mf = ColumnStoreReader(os.path.join(BASE_DIR, \"{}/{}\".format('ModelFeatures', target_name)))\n",
    "BASE_DIR = \"http://data-ssd.comma.life/runner/training_1m/\"\n",
    "\n",
    "vt2 = ColumnStoreReader(os.path.join(BASE_DIR, \"{}/{}\".format('VisionTargets2', target_name)))\n",
    "calib_view = view_from_device.dot(vt2['calib'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "imgs = []\n",
    "pbar = tqdm(enumerate(FrameIterator('cd:/'+target_name.replace(\"|\" , \"/\") + \"/fcamera.hevc\", \"rgb24\")))                                                                                                                                                                                      \n",
    "for fidx, rgb in pbar:\n",
    "  img_o = transform_img(rgb, to_intr=medmodel_intrinsics,                                                                                                                                                                                         \n",
    "                                 output_size=MEDMODEL_INPUT_SIZE,                                                                                                                                                                                                                  \n",
    "                                 calib=calib_view, yuv=False)\n",
    "  w_styles = gen_styles(m, torch.from_numpy(np.array(mf['values'][fidx:fidx+1])).cuda())\n",
    "  img_g = m.G(w_styles, n)\n",
    "  img_g = cv2.resize((np.clip(img_g.cpu().detach().numpy()[0]*256, 0 ,255)).astype(np.uint8).swapaxes(0,2).swapaxes(0,1)[256:], (512, 256))\n",
    "  img = np.vstack((img_o, img_g))\n",
    "  imgs.append(img)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12,12)\n",
    "imshow(imgs[900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    " \n",
    "\n",
    "frameSize = (512, 512)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "out = cv2.VideoWriter('output_video.avi',fourcc, 20, frameSize)\n",
    "\n",
    "for i in range(0,1199):\n",
    "    img = imgs[i][:,:,::-1]\n",
    "    out.write(img)\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
